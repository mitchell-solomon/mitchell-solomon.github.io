---
title: STDE
date: 2025-03-04 15:45:32 -0500
author: mitch
description: understanding stochastic Taylor derivative estimator
categories: [research, learning]
tags: [math, gradients]     # TAG names should always be lowercase
math: true
pin: true
---

# Understanding the Stochastic Taylor Derivative Estimator (STDE) in JAX  
_A Detailed Mathematical and Code Walkthrough_

In many scientific computing and machine learning applications, it is useful to compute differential operators such as the Laplacian (i.e. the trace of the Hessian) of a function. When the dimension is high or the function is complex, forming the full Hessian can be prohibitively expensive. Instead, one can estimate such quantities using randomized methods. One such technique is the **Stochastic Taylor Derivative Estimator (STDE)**, which leverages ideas from Taylor expansions and random projections to approximate contractions of high-order derivative tensors.

In this post, we will:
- Derive the mathematical basis for STDE,
- Explain how random directional derivatives can be used to estimate the trace of the Hessian,
- Present a minimal, generalized implementation in JAX, and
- Discuss how the code connects to the underlying mathematics.

---

## 1. Mathematical Background

### 1.1 Taylor Expansion in a Random Direction

Let \( u: \mathbb{R}^d \to \mathbb{R} \) be a twice-differentiable function. We can write the Taylor expansion of \( u \) around a point \( x \in \mathbb{R}^d \) along a direction \( v \in \mathbb{R}^d \) as:
  
$$
u(x + t\,v) = u(x) + t\,\nabla u(x)^\top v + \frac{t^2}{2}\,v^\top H\,u(x)\,v + \mathcal{O}(t^3),
$$

where:
- \( \nabla u(x) \) is the gradient of \( u \) at \( x \),
- \( H\,u(x) \) is the Hessian (the matrix of second derivatives) at \( x \), and
- \( v^\top H\,u(x)\,v \) is the second directional derivative along \( v \).

Setting \( t = 1 \) (or extracting the coefficient of \( t^2 \)) shows that the second-order term is given by

$$
\frac{1}{2}\,v^\top H\,u(x)\,v.
$$

Multiplying by 2, we isolate the term

$$
v^\top H\,u(x)\,v,
$$

which represents the second directional derivative along \( v \).

### 1.2 Unbiased Estimation of the Trace

The Laplacian of \( u \) at \( x \) is defined as the trace of the Hessian:

$$
\Delta u(x) = \operatorname{trace}(H\,u(x)).
$$

A well-known fact from random matrix theory is that if \( v \) is sampled from any distribution with zero mean and covariance proportional to the identity (i.e. \( \mathbb{E}[v\,v^\top] = I \)), then

$$
\mathbb{E}\Bigl[\,v^\top H\,u(x)\,v\,\Bigr] = \operatorname{trace}(H\,u(x)).
$$

Common choices for \( v \) include:
- **Rademacher vectors:** Each component is independently \( \pm 1 \) with probability 0.5.
- **Standard normal vectors:** Each component is i.i.d. \( \mathcal{N}(0,1) \).

Thus, by sampling many random directions \( v_1, v_2, \dots, v_N \) and computing \( v_i^\top H\,u(x)\,v_i \) for each, averaging these values yields an unbiased estimator of \( \operatorname{trace}(H\,u(x)) \), and hence of the Laplacian.

---

## 2. STDE Using the JAX Jet API

The STDE paper derives the method by considering the Taylor expansion of \( u(x+t\,v) \) and showing that the second-order term is directly related to the contraction \( v^\top H\,u(x)\,v \). JAX provides an experimental API, `jax.experimental.jet`, which can compute these Taylor series expansions automatically.

The key steps are as follows:

1. **Sampling \( v \):**  
   We sample random vectors \( v \) (using a Rademacher distribution, for example, so that each component is \( \pm 1 \)). This ensures that 
   $$\mathbb{E}[v\,v^\top] = I.$$

2. **Computing the Taylor Series:**  
   For each \( v \), we compute the Taylor expansion of \( u(x+t\,v) \) using `jet.jet`. The API returns the series coefficients, from which we extract the second-order term \( v^\top H\,u(x)\,v \).

3. **Averaging:**  
   Averaging the second-order coefficients over many samples gives an unbiased estimator of \( \operatorname{trace}(H\,u(x)) \).

Below is the adapted function using the jet API:

```python
def hess_trace(fn) -> callable:
    def fn_trace(x_i, rng):
        if sparse:
            rng, subkey = jax.random.split(rng)
            idx_set = jax.random.choice(subkey, dim, shape=(rand_batch_size,), replace=False)
            # For sparse sampling, use one-hot vectors.
            rand_vec = jax.vmap(lambda i: jnp.eye(dim)[i])(idx_set)
        else:
            rng, subkey = jax.random.split(rng)
            # Sample v from a Rademacher distribution: each entry is -1 or 1.
            rand_vec = 2 * (jax.random.randint(subkey, shape=(rand_batch_size, dim), minval=0, maxval=2) - 0.5)
        
        # For each random vector v, compute the Taylor series of fn at x_i.
        taylor_2 = lambda v: jet.jet(fun=fn, primals=(x_i,), series=((v, jnp.zeros(dim)),))
        # f_vals are the function values (all equal to fn(x_i)) and hvps are the second-order terms.
        f_vals, (_, hvps) = jax.vmap(taylor_2)(rand_vec)
        trace_est = jnp.mean(hvps)
        if sparse:
            trace_est *= dim
        return f_vals[0], trace_est
    return fn_trace
```

This function returns a new function that, given a point \( x_i \) and a random key, samples many random directions \( v \), computes \( v^\top H\,u(x_i)\,v \) for each, and averages these values to estimate the Hessian trace.

---

## 3. Generalized Implementation in JAX

Below is the complete code, generalized for an arbitrary dimension. The test point \( x \) is constructed as a vector of ones with length given by the variable `dim`. For our function

$$
u(x) = \sum_{i=1}^{\text{dim}} x_i^2,
$$

the exact Laplacian is \(2 \times \text{dim}\).

```python
import jax
import jax.numpy as jnp
from jax.experimental import jet

# General parameters
dim = 5                 # Arbitrary dimension (change as needed)
rand_batch_size = 1000  # Number of random probes
sparse = False          # Use dense sampling (Rademacher distribution)

###########################################################
# Define the scalar function u: R^dim -> R.
# We choose u(x) = sum(x^2), whose Hessian is 2I.
# Hence, the exact Laplacian is 2*dim.
###########################################################
def u(x: jnp.ndarray) -> jnp.ndarray:
    return jnp.sum(x**2)

###########################################################
# STDE using the jet API.
#
# Given a function fn, hess_trace returns a function that,
# for an input x_i and RNG key, does the following:
#
# 1. Samples rand_batch_size random vectors v in R^dim.
#    When sparse is False, each component of v is independently
#    sampled as -1 or 1 (Rademacher distribution).
#
# 2. For each random vector v, computes the Taylor expansion of fn at x_i:
#
#    $$ u(x_i+t\,v) = u(x_i) + t \langle \nabla u(x_i), v \rangle + \frac{t^2}{2}\,v^\top H\,u(x_i)v + \mathcal{O}(t^3). $$
#
# 3. Extracts the second-order coefficient (i.e. \(v^\top H\,u(x_i)v\))
#    and averages over all v to estimate \(\operatorname{trace}(H\,u(x_i))\).
###########################################################
def hess_trace(fn) -> callable:
    def fn_trace(x_i, rng):
        if sparse:
            rng, subkey = jax.random.split(rng)
            idx_set = jax.random.choice(subkey, dim, shape=(rand_batch_size,), replace=False)
            # For sparse sampling, use one-hot vectors.
            rand_vec = jax.vmap(lambda i: jnp.eye(dim)[i])(idx_set)
        else:
            rng, subkey = jax.random.split(rng)
            # Sample v from a Rademacher distribution: each entry is -1 or 1.
            rand_vec = 2 * (jax.random.randint(subkey, shape=(rand_batch_size, dim), minval=0, maxval=2) - 0.5)
        
        # Compute the Taylor series for each random direction v.
        taylor_2 = lambda v: jet.jet(fun=fn, primals=(x_i,), series=((v, jnp.zeros(dim)),))
        # f_vals contains the function values (all equal to u(x_i)), hvps are the second-order terms.
        f_vals, (_, hvps) = jax.vmap(taylor_2)(rand_vec)
        trace_est = jnp.mean(hvps)
        if sparse:
            trace_est *= dim
        return f_vals[0], trace_est
    return fn_trace

###########################################################
# STDE Laplacian Estimator
#
# Given a function u_fn, this function returns an estimate of
# the Laplacian (i.e. the trace of the Hessian) at point x.
###########################################################
def stde_laplacian(u_fn, x: jnp.ndarray, key: jnp.ndarray) -> jnp.ndarray:
    fn_trace = hess_trace(u_fn)
    _, trace_est = fn_trace(x, key)
    return trace_est

###########################################################
# Main: Generalized to arbitrary dimension.
#
# We generate a test point x as a vector of ones with length 'dim'
# and compare the STDE Laplacian estimate to the exact value \(2\cdot\text{dim}\).
###########################################################
def main():
    # Create a test point x in R^dim.
    x = jnp.ones((dim,))
    key = jax.random.PRNGKey(0)
    
    approx_lap = stde_laplacian(u, x, key)
    exact_lap = 2.0 * dim
    
    print(f"Dimension: {dim}")
    print(f"STDE Approx Laplacian: {approx_lap:.4f}")
    print(f"Exact Laplacian:       {exact_lap:.4f}")

if __name__ == "__main__":
    main()
```

### Code Walkthrough

1. **Parameter Setup:**  
   The variable `dim` controls the problem's dimension. We generate random vectors \(v\) in \(\mathbb{R}^{\text{dim}}\) using a Rademacher distribution (when `sparse` is False). The number of samples is set by `rand_batch_size`.

2. **Defining \( u(x) \):**  
   The function \( u(x) = \sum_{i=1}^{\text{dim}} x_i^2 \) has a Hessian equal to \(2I\), so the Laplacian is \(2 \times \text{dim}\).

3. **Hessian Trace Estimation:**  
   The function `hess_trace` uses the JAX jet API to compute the Taylor series of \( u(x_i + t\,v) \) for each random vector \(v\). The second-order term extracted from the series is \(v^\top H\,u(x_i)\,v\). Averaging these over all samples provides an unbiased estimator of the trace of the Hessian.

4. **Generalization and Testing:**  
   The test point \( x \) is generated as a vector of ones with length equal to `dim`. We then compare the estimated Laplacian to the exact value \( 2\cdot\text{dim} \).

---

## 4. Conclusion

The Stochastic Taylor Derivative Estimator (STDE) provides an efficient and mathematically elegant way to approximate the trace of the Hessian without forming the full second derivative tensor. By leveraging the Taylor series expansion along random directions and ensuring that the random vectors satisfy

$$
\mathbb{E}[v\,v^\top] = I,
$$

we obtain an unbiased estimator for the Laplacian (and more generally, for any contraction of the derivative tensor). Our implementation in JAX uses the jet API to extract the second-order coefficient and averages it over many samples, generalizing seamlessly to an arbitrary dimension.

I hope this post has clarified the mathematical ideas behind STDE and shown you how to implement it practically using JAX. Happy coding and exploring advanced differentiation techniques!

---

_References:_  
- The STDE paper (provided by the author) [\( \text{[cite]} \)]  
- [JAX Documentation: jet API](https://jax.readthedocs.io/en/latest/jax.experimental.jet.html)  
- Related blog posts on randomized trace estimation techniques

Feel free to comment below with any questions or further insights!