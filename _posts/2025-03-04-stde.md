---
title: STDE
date: 2025-03-04 15:45:32 -0500
categories: [posting, subposting]
tags: [writing]     # TAG names should always be lowercase
---


# Understanding the Stochastic Taylor Derivative Estimator (STDE) in JAX  
_A detailed mathematical and code walkthrough_

In many scientific computing and machine learning applications, it is useful to compute differential operators such as the Laplacian (i.e. the trace of the Hessian) of a function. When the dimension is high or the function is complex, forming the full Hessian can be prohibitively expensive. Instead, one can estimate such quantities using randomized methods. One such technique is the **Stochastic Taylor Derivative Estimator (STDE)**, which leverages ideas from Taylor expansions and random projections to approximate contractions of high-order derivative tensors.

In this blog post, we will:
- Derive the mathematical basis for STDE,
- Explain how random directional derivatives can be used to estimate the trace of the Hessian,
- Present a minimal, generalized implementation in JAX, and
- Discuss how the code connects to the underlying mathematics.

---

## 1. Mathematical Background

### 1.1 Taylor Expansion in a Random Direction

Let \( u: \mathbb{R}^d \to \mathbb{R} \) be a twice-differentiable function. We can write the Taylor expansion of \( u \) around a point \( x \in \mathbb{R}^d \) along a direction \( v \in \mathbb{R}^d \) as:

\[
u(x+t\,v) = u(x) + t\,\nabla u(x)^\top v + \frac{t^2}{2}\,v^\top H u(x) \, v + \mathcal{O}(t^3),
\]

where:
- \( \nabla u(x) \) is the gradient of \( u \) at \( x \),
- \( H u(x) \) is the Hessian (the matrix of second derivatives) at \( x \), and
- \( v^\top H u(x) v \) is the second directional derivative along \( v \).

When we set \( t=1 \) (or more generally, look at the coefficient of \( t^2 \)) and ignore higher-order terms, the second-order term is given by:

\[
\frac{1}{2}\,v^\top H u(x) \, v.
\]

Multiplying by 2 (or equivalently, focusing on the quantity \( v^\top H u(x) v \)) isolates the second directional derivative. 

### 1.2 Unbiased Estimation of the Trace

The Laplacian of \( u \) at \( x \) is defined as the trace of the Hessian:

\[
\Delta u(x) = \operatorname{trace}(H u(x)).
\]

A well-known fact from random matrix theory is that if \( v \) is sampled from any distribution with zero mean and covariance proportional to the identity (i.e. \(\mathbb{E}[v\,v^\top] = I\)), then:

\[
\mathbb{E}[v^\top H u(x) \, v] = \operatorname{trace}(H u(x)).
\]

Common choices for \( v \) include:
- **Rademacher vectors:** Each component is independently \(\pm 1\) with probability 0.5.
- **Standard normal vectors:** Each component is i.i.d. \(\mathcal{N}(0,1)\).

Thus, by sampling a number of such random directions \( v_1, v_2, \dots, v_N \) and computing \( v_i^\top H u(x) \, v_i \) for each, averaging over these estimates yields an unbiased estimator of \( \operatorname{trace}(H u(x)) \), and hence the Laplacian.

---

## 2. STDE Using the JAX Jet API

The original STDE paper derives the method by considering the Taylor expansion of \( u(x+t\,v) \) and then showing that the second-order term is directly related to the contraction of the Hessian with the random vector \( v \). One can compute these terms automatically using higher-order automatic differentiation.

JAX provides an experimental API, `jax.experimental.jet`, that can compute these Taylor series expansions. The code snippet below, adapted from the author's implementation, computes the Hessian trace as follows:
  
- **Sampling \( v \):**  
  We sample random vectors \( v \) either densely or sparsely. When using dense sampling, a common choice is the Rademacher distribution (each component \(\pm 1\)). This ensures that  
  \(\mathbb{E}[v\,v^\top] = I\).

- **Computing the Taylor Series:**  
  For each \( v \), we compute the series expansion of \( u(x + t\,v) \) and extract the second-order coefficient, which is \( v^\top H u(x) v \).

- **Averaging:**  
  Averaging these estimates over all sampled \( v \) yields an unbiased estimator for the Laplacian.

Below is our implementation of the `hess_trace` function using the jet API.

```python
def hess_trace(fn) -> callable:
    def fn_trace(x_i, rng):
        if sparse:
            rng, subkey = jax.random.split(rng)
            idx_set = jax.random.choice(subkey, dim, shape=(rand_batch_size,), replace=False)
            # For sparse sampling, use one-hot vectors.
            rand_vec = jax.vmap(lambda i: jnp.eye(dim)[i])(idx_set)
        else:
            rng, subkey = jax.random.split(rng)
            # Sample v from a Rademacher distribution: each entry is -1 or 1.
            rand_vec = 2 * (jax.random.randint(subkey, shape=(rand_batch_size, dim), minval=0, maxval=2) - 0.5)
        
        # For each random vector v, compute the Taylor series of fn at x_i.
        taylor_2 = lambda v: jet.jet(fun=fn, primals=(x_i,), series=((v, jnp.zeros(dim)),))
        # f_vals are the function values (all equal to fn(x_i)) and hvps are the second-order terms.
        f_vals, (_, hvps) = jax.vmap(taylor_2)(rand_vec)
        trace_est = jnp.mean(hvps)
        if sparse:
            trace_est *= dim
        return f_vals[0], trace_est
    return fn_trace
```

This function, `hess_trace`, wraps a function \( \texttt{fn} \) (in our case, \( u \)) and returns a new function that takes a point \( x_i \) and a random key, then:
- Samples many random directions \( v \),
- Uses `jet.jet` to compute the second-order Taylor series term for each \( v \), and
- Averages these second-order coefficients to estimate the trace of the Hessian.

---

## 3. Generalized Implementation in JAX

Below is the complete generalized example that takes an arbitrary dimension. The test point \( x \) is constructed as a vector of ones with length given by the variable `dim`. For our test function  
\[
u(x)=\sum_{i=1}^{\text{dim}} x_i^2,
\]
the exact Laplacian is \(2 \cdot \text{dim}\).

```python
import jax
import jax.numpy as jnp
from jax.experimental import jet

# General parameters
dim = 5                 # Arbitrary dimension (change as needed)
rand_batch_size = 1000  # Number of random probes
sparse = False          # Use dense sampling (Rademacher distribution)

###########################################################
# Define the scalar function u: R^dim -> R.
# We choose u(x) = sum(x^2), whose Hessian is 2*I.
# Hence, the exact Laplacian is 2*dim.
###########################################################
def u(x: jnp.ndarray) -> jnp.ndarray:
    return jnp.sum(x**2)

###########################################################
# STDE using the jet API.
#
# Given a function fn, hess_trace returns a function that,
# for an input x_i and RNG key, does the following:
#
# 1. Samples rand_batch_size random vectors v in R^dim.
#    When sparse is False, each component of v is independently
#    sampled as -1 or 1 (Rademacher distribution).
#
# 2. For each random vector v, computes the Taylor expansion of fn at x_i:
#       fn(x_i + t*v) = fn(x_i) + t <grad fn(x_i), v> + ½ t^2 (vᵀ H fn(x_i) v) + O(t³)
#
# 3. Extracts the second-order coefficient (vᵀ H fn(x_i) v)
#    and averages over all v to estimate trace(H fn(x_i)).
###########################################################
def hess_trace(fn) -> callable:
    def fn_trace(x_i, rng):
        if sparse:
            rng, subkey = jax.random.split(rng)
            idx_set = jax.random.choice(subkey, dim, shape=(rand_batch_size,), replace=False)
            # For sparse sampling, use one-hot vectors.
            rand_vec = jax.vmap(lambda i: jnp.eye(dim)[i])(idx_set)
        else:
            rng, subkey = jax.random.split(rng)
            # Sample v from a Rademacher distribution: each entry is -1 or 1.
            rand_vec = 2 * (jax.random.randint(subkey, shape=(rand_batch_size, dim), minval=0, maxval=2) - 0.5)
        
        # For each random vector v, compute the Taylor series of fn at x_i.
        taylor_2 = lambda v: jet.jet(fun=fn, primals=(x_i,), series=((v, jnp.zeros(dim)),))
        # f_vals are the function values (all equal to fn(x_i)) and hvps are the second-order terms.
        f_vals, (_, hvps) = jax.vmap(taylor_2)(rand_vec)
        trace_est = jnp.mean(hvps)
        if sparse:
            trace_est *= dim
        return f_vals[0], trace_est
    return fn_trace

###########################################################
# STDE Laplacian Estimator
#
# Given a function u_fn, this function returns an estimate of
# the Laplacian (i.e. the trace of the Hessian) at point x.
###########################################################
def stde_laplacian(u_fn, x: jnp.ndarray, key: jnp.ndarray) -> jnp.ndarray:
    fn_trace = hess_trace(u_fn)
    _, trace_est = fn_trace(x, key)
    return trace_est

###########################################################
# Main: Generalized to arbitrary dimension.
#
# We generate a test point x as a vector of ones with length 'dim'
# and compare the STDE Laplacian estimate to the exact value (2*dim).
###########################################################
def main():
    # Create a test point x in R^dim.
    x = jnp.ones((dim,))
    key = jax.random.PRNGKey(0)
    
    approx_lap = stde_laplacian(u, x, key)
    exact_lap = 2.0 * dim
    
    print(f"Dimension: {dim}")
    print(f"STDE Approx Laplacian: {approx_lap:.4f}")
    print(f"Exact Laplacian:       {exact_lap:.4f}")

if __name__ == "__main__":
    main()
```

### Code Walkthrough

1. **Parameter Setup:**  
   We set `dim` to control the dimension of the problem and `rand_batch_size` to control the number of random probes used in the estimator.

2. **Defining \( u(x) \):**  
   Our test function is \( u(x) = \sum_{i=1}^{\text{dim}} x_i^2 \). Since the Hessian of this function is \( 2I \), the exact Laplacian is \( 2 \cdot \text{dim} \).

3. **hess_trace Implementation:**  
   This function uses the JAX jet API to compute the Taylor expansion of \( u(x+t\,v) \) for a given random vector \( v \).  
   - When `sparse` is False, we sample \( v \) from a Rademacher distribution (each component being either \(-1\) or \(1\)).  
   - For each \( v \), we compute the second-order term using `jet.jet`.  
   - The average of these second-order terms gives us an unbiased estimator of the Hessian trace.

4. **Estimating the Laplacian:**  
   The `stde_laplacian` function wraps `hess_trace` and extracts the trace estimate.

5. **Generalization and Testing:**  
   In the `main` function, we generate a test vector \( x \) in \( \mathbb{R}^{\text{dim}} \) and compare the STDE-based Laplacian with the exact value \( 2\cdot\text{dim} \).

---

## 4. Conclusion

The Stochastic Taylor Derivative Estimator (STDE) provides a powerful way to approximate the trace of the Hessian (and other tensor contractions) without forming the entire derivative tensor explicitly. By leveraging the Taylor series expansion along random directions—where the random vectors are chosen to have zero mean and identity covariance—we obtain an unbiased estimator of the Laplacian. 

This approach, as demonstrated in our JAX implementation, is both computationally efficient and mathematically elegant. The use of JAX’s jet API allows us to automatically compute higher-order derivatives, and by averaging over many random probes, we can closely approximate the desired quantity even in high dimensions.

I hope this post has clarified the mathematics behind STDE and shown you how to implement it in practice. Feel free to experiment with different functions \( u \), dimensions, and random vector distributions to further explore this versatile technique!

---

_References:_  
- STDE Paper [provided by the author] [citeturn0file0]  
- JAX Documentation on [jet](https://jax.readthedocs.io/en/latest/jax.experimental.jet.html)  
- Related blog posts on randomized trace estimation techniques

Happy coding and exploring advanced differentiation techniques in JAX!